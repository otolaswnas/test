@app.route("/healthz")
def health():
    target_url = "http://apm-server:8888/stats"
    try:
        # Wykonujemy tylko szybkie zapytanie HEAD lub GET z krótkim timeoutem
        r = requests.get(target_url, timeout=2)
        r.raise_for_status()
        return "OK - APM Server reachable", 200
    except Exception as e:
        logger.warning(f"Healthcheck failed: APM Server at {target_url} is unreachable: {e}")
        return "Service Unavailable - APM Server down", 503

###################################

# Sonda gotowości: Sprawdza czy APM Server odpowiada
          readinessProbe:
            httpGet:
              path: /healthz
              port: metrics
            initialDelaySeconds: 10 # Daj APM Serverowi 10s na start
            periodSeconds: 10       # Sprawdzaj co 10s
            failureThreshold: 3     # Wyłącz ruch po 3 nieudanych próbach
          
          # Sonda życia: Sprawdza tylko czy sam proces Pythona nie zawisł
          livenessProbe:
            httpGet:
              path: /healthz # Możesz użyć tego samego endpointu lub stworzyć osobny /alive
              port: metrics
            initialDelaySeconds: 15
            periodSeconds: 20




$$$$$$$$$$$$$$$$$$$$$$$



import requests
import logging
import sys
from flask import Flask
from waitress import serve

# Konfiguracja logowania do stdout (widoczne w oc logs)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

def flatten_json(y):
    out = {}
    def flatten(x, name=''):
        if isinstance(x, dict):
            for a in x:
                flatten(x[a], name + a + '_')
        else:
            out[name[:-1]] = x
    flatten(y)
    return out

@app.route("/metrics")
def metrics():
    target_url = "http://apm-server:8888/stats"
    try:
        logger.info(f"Scraping metrics from {target_url}")
        r = requests.get(target_url, timeout=5)
        r.raise_for_status() # Rzuć błąd jeśli status != 200
        
        data = flatten_json(r.json())
        output = []
        for key, value in data.items():
            if isinstance(value, (int, float, bool)):
                clean_key = key.replace(".", "_").replace("-", "_")
                output.append(f"apm_stat_{clean_key} {float(value)}")
        
        logger.info(f"Successfully exported {len(output)} metrics")
        return "\n".join(output), 200, {"Content-Type": "text/plain"}
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Error connecting to APM Server: {e}")
        return f"# Error reaching APM Server: {str(e)}", 500
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return f"# Internal Error: {str(e)}", 500

@app.route("/healthz")
def health():
    return "OK", 200

if __name__ == "__main__":
    logger.info("Starting APM JSON Exporter on port 8888")
    serve(app, host="0.0.0.0", port=8888)


################################################################333
{
  "annotations": { "list": [ { "builtIn": 1, "datasource": "-- Grafana --", "enable": true, "hide": true, "iconColor": "rgba(0, 211, 255, 1)", "name": "Annotations & Alerts", "type": "dashboard" } ] },
  "editable": true, "fiscalYearStartMonth": 0, "graphTooltip": 0, "links": [], "liveNow": false,
  "panels": [
    {
      "title": "Kafka Output Status (Acked Events)",
      "type": "gauge", "gridPos": { "h": 8, "w": 6, "x": 0, "y": 0 },
      "targets": [ { "expr": "sum(rate(apm_stat_metrics_libbeat_output_events_acked[1m]))", "refId": "A" } ],
      "fieldConfig": { "defaults": { "min": 0, "unit": "reqps", "thresholds": { "mode": "absolute", "steps": [ { "color": "red", "value": null }, { "color": "green", "value": 0.1 } ] } } }
    },
    {
      "title": "Internal Pipeline Queue (Backpressure)",
      "type": "timeseries", "gridPos": { "h": 8, "w": 9, "x": 6, "y": 0 },
      "targets": [ { "expr": "apm_stat_metrics_libbeat_pipeline_events_active", "legendFormat": "Events in Memory", "refId": "A" } ],
      "fieldConfig": { "defaults": { "custom": { "drawStyle": "line", "fillOpacity": 20 }, "unit": "short" } }
    },
    {
      "title": "Kafka Errors (Failed Events)",
      "type": "timeseries", "gridPos": { "h": 8, "w": 9, "x": 15, "y": 0 },
      "targets": [ { "expr": "sum(rate(apm_stat_metrics_libbeat_output_events_failed[1m]))", "legendFormat": "Errors/s", "refId": "A" } ],
      "fieldConfig": { "defaults": { "color": { "mode": "fixed", "fixedColor": "red" }, "unit": "reqps" } }
    },
    {
      "title": "Input (Apps) vs Output (Kafka)",
      "type": "timeseries", "gridPos": { "h": 10, "w": 24, "x": 0, "y": 8 },
      "targets": [
        { "expr": "sum(rate(apm_stat_metrics_apm_server_processor_events_accepted[1m]))", "legendFormat": "Accepted from Apps", "refId": "A" },
        { "expr": "sum(rate(apm_stat_metrics_libbeat_output_events_acked[1m]))", "legendFormat": "Written to Kafka", "refId": "B" }
      ],
      "fieldConfig": { "defaults": { "custom": { "lineWidth": 2 }, "unit": "reqps" } }
    }
  ],
  "refresh": "10s", "schemaVersion": 36, "style": "dark", "tags": ["apm", "kafka", "ocp"], "templating": { "list": [] }, "time": { "from": "now-1h", "to": "now" }, "title": "APM Server to Kafka Pipeline", "uid": "apm_kafka_monitor"
}


########################3

import requests
from flask import Flask
from waitress import serve # Dodaj to

app = Flask(__name__)

def flatten_json(y, prefix=''):
    out = {}
    def flatten(x, name=''):
        if isinstance(x, dict):
            for a in x:
                flatten(x[a], name + a + '_')
        elif isinstance(x, (int, float, bool)):
            out[name[:-1]] = x
    flatten(y, prefix)
    return out

@app.route("/metrics")
def metrics():
    try:
        # Pamiętaj o poprawnej nazwie serwisu APM
        r = requests.get("http://apm-server:8888/stats", timeout=5)
        data = flatten_json(r.json())
        output = []
        for key, value in data.items():
            clean_key = key.replace(".", "_").replace("-", "_")
            output.append(f"apm_stat_{clean_key} {float(value)}")
        return "\n".join(output), 200, {"Content-Type": "text/plain"}
    except Exception as e:
        return f"# Error: {str(e)}", 500

if __name__ == "__main__":
    # Użycie Waitress zamiast app.run() usuwa ostrzeżenie i jest bezpieczniejsze
    serve(app, host="0.0.0.0", port=8888)


$$$$$$$$$$$$$$$$$$$$$$$$$$$
import requests
from flask import Flask

app = Flask(__name__)

def flatten_json(y):
    out = {}
    def flatten(x, name=''):
        if type(x) is dict:
            for a in x:
                flatten(x[a], name + a + '_')
        else:
            out[name[:-1]] = x
    flatten(y)
    return out

@app.route("/metrics")
def metrics():
    try:
        # Zmień 'apm-server' na nazwę swojego Serwisu APM w OCP
        r = requests.get("http://apm-server:8888/stats", timeout=5)
        data = flatten_json(r.json())
        prometheus_metrics = []
        for key, value in data.items():
            if isinstance(value, (int, float, bool)):
                # Czyścimy nazwy pod format Prometheusa
                clean_key = key.replace(".", "_").replace("-", "_")
                prometheus_metrics.append(f"apm_stat_{clean_key} {float(value)}")
        return "\n".join(prometheus_metrics), 200, {"Content-Type": "text/plain"}
    except Exception as e:
        return f"# Error: {str(e)}", 500

if __name__ == "__main__":
    # Musi być 0.0.0.0, żeby OCP widział kontener
    app.run(host="0.0.0.0", port=8888)


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

apiVersion: v1
kind: Service
metadata:
  name: apm-json-exporter
spec:
  selector:
    app: apm-json-exporter
  ports:
    - name: metrics
      port: 8888
      targetPort: 8888




apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-json-exporter
data:
  exporter.py: |
    from flask import Flask
    import requests

    app = Flask(__name__)

    @app.route("/metrics")
    def metrics():
        data = requests.get("http://apm-server:8888/stats").json()
        out = []
        for key, value in data.items():
            if isinstance(value, (int, float)):
                out.append(f"apmserver_{key} {value}")
        return "\n".join(out), 200, {"Content-Type": "text/plain"}



apiVersion: apps/v1
kind: Deployment
metadata:
  name: apm-json-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apm-json-exporter
  template:
    metadata:
      labels:
        app: apm-json-exporter
    spec:
      containers:
        - name: exporter
          image: python:3.11-slim
          command: ["python", "/app/exporter.py"]
          ports:
            - containerPort: 8888
              name: metrics
          volumeMounts:
            - name: exporter-code
              mountPath: /app
      volumes:
        - name: exporter-code
          configMap:
            name: apm-json-exporter


#######################
#######################3
########################3
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  otel-collector-config.yaml: |
    receivers:
      # Receiver, który pobiera JSON z APM Servera
      http_client:
        endpoint: "http://apm-server:8888/stats"
        method: GET
        collection_interval: 10s

    processors:
      # Parsuje JSON do atrybutów
      json_parser:
        parse_from: body
        parse_to: attributes

      # Zamienia każde pole numeryczne na metrykę Prometheus
      transform:
        metric_statements:
          - context: datapoint
            statements:
              - set(metric.name, "apmserver_" + attributes["key"]) where attributes["value"] != nil
              - set(metric.value_double, attributes["value"]) where attributes["value"] != nil

      batch: {}

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8888"

    service:
      pipelines:
        metrics:
          receivers: [http_client]
          processors: [json_parser, transform, batch]
          exporters: [prometheus]


$$$$$$$$$$$$$$$$

apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  otel-collector-config.yaml: |
    receivers:
      http:
        endpoint: "0.0.0.0:9000"
        include_metadata: false

    processors:
      json_parser:
        parse_from: body
        parse_to: attributes
        if: 'body matches "^\\{"'

      transform:
        metric_statements:
          - context: datapoint
            statements:
              - set(metric.name, "apmserver_" + attributes["key"]) where attributes["value"] != nil
              - set(metric.value_double, attributes["value"]) where attributes["value"] != nil

      batch: {}

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8888"

    service:
      pipelines:
        metrics:
          receivers: [http]
          processors: [json_parser, transform, batch]
          exporters: [prometheus]






##################################################33

apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  otel-collector-config.yaml: |
    receivers:
      # Pobieramy JSON z APM Servera
      httpcheck:
        targets:
          - endpoint: "http://apm-server:8888/stats"
        method: GET
        collection_interval: 10s

    processors:
      batch: {}

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8888"

    service:
      pipelines:
        metrics:
          receivers: [httpcheck]
          processors: [batch]
          exporters: [prometheus]


@@@@@@@@@@@@@@@@@@@@2

apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  labels:
    app: otel-collector
spec:
  selector:
    app: otel-collector
  ports:
    - name: metrics
      port: 8888
      targetPort: 8888



#######################################################

apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.102.0
          args:
            - "--config=/etc/otel/otel-collector-config.yaml"
          ports:
            - containerPort: 8888
              name: metrics
          volumeMounts:
            - name: config
              mountPath: /etc/otel/otel-collector-config.yaml
              subPath: otel-collector-config.yaml
      volumes:
        - name: config
          configMap:
            name: otel-collector-config



##############################################################3
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metricbeat
  labels:
    app: metricbeat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metricbeat
  template:
    metadata:
      labels:
        app: metricbeat
    spec:
      containers:
        - name: metricbeat
          image: docker.elastic.co/beats/metricbeat:8.12.0
          args: ["-e"]
          ports:
            - containerPort: 8888
              name: metrics
          volumeMounts:
            - name: config
              mountPath: /usr/share/metricbeat/metricbeat.yml
              subPath: metricbeat.yml
      volumes:
        - name: config
          configMap:
            name: metricbeat-config

------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: metricbeat-config
data:
  metricbeat.yml: |
    metricbeat.modules:
      - module: http
        metricsets: ["json"]
        hosts: ["http://apm-server:8888/stats"]
        namespace: "apmserver"
        period: 10s

    output.prometheus:
      enabled: true
      host: "0.0.0.0"
      port: 8888


-----------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: metricbeat
  labels:
    app: metricbeat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metricbeat
  template:
    metadata:
      labels:
        app: metricbeat
    spec:
      containers:
        - name: metricbeat
          image: docker.elastic.co/beats/metricbeat:8.12.0
          args: ["-e"]
          ports:
            - containerPort: 8888
              name: metrics
          volumeMounts:
            - name: config
              mountPath: /usr/share/metricbeat/metricbeat.yml
              subPath: metricbeat.yml
      volumes:
        - name: config
          configMap:
            name: metricbeat-config




# Sekcja 1: Otwiera port 8888 (Serwer HTTP)
http.enabled: true
http.host: "0.0.0.0"
http.port: 8888

# Sekcja 2: Włącza format Prometheus (Wspólne dla wszystkich Beats 8.x)
monitoring.metrics.prometheus.enabled: true
(((((((((((((((((((((((((


monitoring:
      enabled: true
      http:
        enabled: true
        host: 0.0.0.0
        port: 8888
4444444444444444
oc exec NAZWA_PODA_APM -- bash -c "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/8888' && echo 'Port Otwarty' || echo 'Port Zamknięty'"

333


Masz rację — pomyliłem nazwy pól. W 8.14.3:

- W _cat/nodes nie ma fs.used_percent. Szukaj dysku jako disk.used_percent.
- http.current_open nie jest kolumną w _cat/nodes — to metryka z _nodes/stats (sekcja http).

Podaję poprawne przykłady:

- Użycie dysku per węzeł (prosto i czytelnie)
  GET _cat/allocation?v&h=node,shards,disk.percent,disk.used,disk.avail,disk.total

- Alternatywnie w _cat/nodes (plus CPU/heap)
  GET _cat/nodes?v&h=name,ip,node.role,heap.percent,ram.percent,cpu,load_1m,disk.used_percent,master

- Otwarte połączenia HTTP (tu jest http.current_open)
  GET _nodes/stats/http?filter_path=nodes.*.name,nodes.*.http.current_open,nodes.*.http.total_opened

- Kolejki/odrzucenia (write/bulk/ingest)
  GET _cat/thread_pool/write,bulk,ingest?v&h=node_name,name,active,queue,rejected

Wskazówka: zawsze możesz podejrzeć dostępne kolumny tak:
- GET _cat/nodes?help
- GET _cat/thread_pool?help

Jeśli chcesz, podeślę też gotowy zestaw “kopiuj-wklej” pod konkretny problem (timeouty Logstasha), ale najpierw dobrze mieć te właściwe metryki.

################################3

Socket timeout w Logstash zwykle znaczy: Logstash wysłał żądanie do Elasticsearch i nie dostał odpowiedzi w czasie (Manticore::SocketTimeout). Najczęstsze przyczyny: przeciążony ES (CPU/GC/dysk), kolejki w thread poolach, wąskie gardła na LB/proxy, albo zbyt duże/ciężkie bulki.

Poniżej szybka checklista: co sprawdzić i jak to zinterpretować.

Szybka diagnoza z hosta Logstash
- Zmierz connect i time-to-first-byte:
  curl -u user:pass -sS -o /dev/null -w 'connect=%{time_connect}s ttfb=%{time_starttransfer}s total=%{time_total}s code=%{http_code}\n' https://ES:9200/_cluster/health?timeout=10s
  - connect duży -> DNS/sieć/LB/firewall.
  - ttfb duży lub timeout -> ES nie nadąża lub coś po drodze ucina/przetrzymuje.
- Jeśli masz LB/proxy (NGINX/ALB/HAProxy): porównaj wyniki bezpośrednio na port 9200 węzła vs przez LB. Częsty winny: zbyt krótki idle/proxy_read_timeout.

Co sprawdzić po stronie Elasticsearch (API)
- Ogólne zdrowie/obciążenie:
  GET _cat/nodes?v&h=name,ip,role,heap.percent,ram.percent,cpu,load_1m,fs.used_percent,http.current_open,master
  Szukaj: heap.percent > 85, cpu ~100, fs.used_percent bardzo wysoki, tysiące http.current_open.
- Kolejki i odrzucenia (backpressure):
  GET _cat/thread_pool/write?v&h=node_name,name,active,queue,rejected
  Jeśli queue > 0 lub rosnące rejected > 0, węzeł nie nadąża z indeksowaniem.
  Dodatkowo: GET _cat/thread_pool/ingest?v oraz search, bulk (w zależności od wersji).
- Statystyki węzłów (szczegóły):
  GET _nodes/stats/jvm,os,process,fs,indices,thread_pool,http,ingest?pretty
  - jvm.mem.heap_used_percent, długie jvm.gc.collectors.old.collection_time_in_millis
  - os.cpu.load_average i iowait (jeśli wysoki → dysk wąskie gardło)
  - http.current_open, http.total_opened (czy nie ma wycieków połączeń)
  - indices.indexing.* (czy index_current stale wysokie)
  - ingest.total i pipelines.*.metrics (czy ingest pipeline’y nie dławią)
- Gorące wątki (czy coś „mieli” CPU):
  GET _nodes/hot_threads?threads=3&ignore_idle_threads=true
- Zadania w kolejce klastra:
  GET _cluster/pending_tasks
- Logi Elasticsearch na węzłach
  Szukaj: CircuitBreakingException, rejected execution, flood-stage disk watermark, long GC, failed to parse pipeline, rejecting mapping update, 429/EsRejectedExecutionException.

Typowe przyczyny i szybkie działania
- Kolejki write/ingest pełne, rejected > 0:
  - Zmniejsz rozmiar bulk (w Logstash: zmniejsz batch/bulk_max_size), więcej mniejszych żądań.
  - Rozłóż ruch na więcej węzłów (hosts => ["es1","es2","es3"]).
  - Skaluj zasoby/liczbę data/ingest nodes. Krótkoterminowo można zwiększyć queue_size dla thread_pool.write, ale lepiej usunąć przyczynę.
- Wysokie GC/heap:
  - Upewnij się, że Xms = Xmx i nie więcej niż ~50% RAM, bez swapu.
  - Ogranicz liczbę pól/mapping explosions, przemyśl dynamic_templates.
  - Rozważ podniesienie refresh_interval na indeksach przy ciężkim indeksowaniu (np. 10–30s) i replik 0 podczas masowego załadowania.
- I/O dyskowe wąskie gardło:
  - SSD, więcej IOPS, ogranicz liczbę shardów (za dużo shardów = dużo merge).
  - Sprawdź czy nie ma flood-stage disk watermark (wtedy indeksowanie może być blokowane).
- Ingest pipeline’y ciężkie:
  - Profiluj pipelines (GET _nodes/stats/ingest), przenieś cięższe transformacje do Logstash, cache’uj grok/kv jeśli możliwe, upraszczaj painless.
- Za dużo lub zrywane połączenia HTTP:
  - W Logstash rozważ: pool_max, pool_max_per_route, podaj wiele hosts. Upewnij się, że keepalive nie gryzie się z LB (zwiększ idle/proxy_read_timeout na LB).
- LB/proxy timeouts:
  - NGINX: proxy_read_timeout, proxy_send_timeout > request_timeout Logstasha.
  - ALB/ELB: podnieś Idle timeout.
- Blokady klastra:
  - Sprawdź, czy indeksy nie są read-only (po flood-stage): GET _all/_settings/index.blocks*.

Tuning po stronie Logstash (przykłady)
- Wydłuż timeout na odpowiedź gdy ES bywa ociężały:
  request_timeout => 120
- Zmniejsz batch (skraca czas przetwarzania jednego bulk):
  bulk_max_size => 200   (wartość dobra na start; dopasuj do środowiska)
- Rozsądna pula połączeń i wiele hostów:
  hosts => ["https://es1:9200","https://es2:9200","https://es3:9200"]
  pool_max => 1000
  pool_max_per_route => 100
- Jeśli masz proxy/LB, wyłącz sniffing (żeby nie odkrywał prywatnych adresów):
  sniffing => false

Jak zdiagnozować na szybko co boli
- Jeśli curl pokazuje długi connect: sieć/LB.
- Jeśli connect szybki, a ttfb wolny: ES przeciążony → patrz thread_pool write, GC, IO, ingest.
- Jeśli są rejected w thread_pool: zmniejsz batch, rozłóż ruch, skaluj ES.
- Jeśli http.current_open bardzo wysokie i rośnie: tunuj pool w Logstash i/lub LB idle timeout.

Jeśli podeślesz:
- dokładny log z Logstash (linia z Manticore::SocketTimeout i kilka poprzednich),
- wynik: _cat/thread_pool/write oraz fragment _nodes/stats (jvm/os/http) z czasu problemu,
- i czy wchodzisz przez LB/proxy,
to podpowiem bardziej precyzyjnie co poprawić.
